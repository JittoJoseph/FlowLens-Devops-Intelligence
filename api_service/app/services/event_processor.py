# api_service/app/services/event_processor.py

import json
from typing import Set
from datetime import datetime
from loguru import logger
from app.data.database import db_helpers
from app.services import ai_service
from app.services.websocket_manager import websocket_manager

# A simple in-memory lock to prevent race conditions
PROCESSING_EVENTS: Set[str] = set()


def _serialize_datetime_fields(data: dict) -> dict:
    """Convert datetime objects to ISO format strings for JSON serialization."""
    serialized = {}
    for key, value in data.items():
        if isinstance(value, datetime):
            serialized[key] = value.isoformat()
        else:
            serialized[key] = value
    return serialized


async def process_new_pull_request(pr_record: dict):
    """
    Process a new or updated pull request.
    Generates AI insights only for new PRs with files_changed data.
    For updates (status changes, approvals, etc.), just broadcasts the updated state.
    """
    repo_id = pr_record['repo_id']
    pr_number = pr_record['pr_number']
    record_id = pr_record['id']
    
    if record_id in PROCESSING_EVENTS:
        logger.warning(f"PR record {record_id} is already being processed. Skipping.")
        return
    
    PROCESSING_EVENTS.add(record_id)
    
    try:
        logger.info(f"Processing PR #{pr_number} in repository {repo_id}")
        
        # Check if this is a genuinely new PR or just a status update
        files_changed = pr_record.get('files_changed', [])
        existing_insights = await db_helpers.select(
            "insights",
            where={"repo_id": repo_id, "pr_number": pr_number},
            limit=1
        )
        
        # Only generate insights if:
        # 1. This PR has files_changed data AND
        # 2. No insights exist yet (meaning this is a new PR, not just a status update)
        should_generate_insights = files_changed and not existing_insights
        
        if should_generate_insights:
            logger.info(f"New PR #{pr_number} detected with {len(files_changed)} changed files, generating AI analysis...")
            await _generate_ai_insight_for_pr(pr_record)
            # Broadcast as new PR with insight generation
            await websocket_manager.broadcast_pr_state_update(repo_id, pr_number)
        elif existing_insights:
            logger.info(f"PR #{pr_number} update detected (status/approval change), broadcasting updated state...")
            # Broadcast as state update only
            await websocket_manager.broadcast_pr_state_update(repo_id, pr_number)
        else:
            logger.info(f"PR #{pr_number} has no file changes, skipping insight generation...")
            # Broadcast as basic update
            await websocket_manager.broadcast_pr_state_update(repo_id, pr_number)
        
    except Exception as e:
        logger.error(f"Failed to process PR #{pr_number} in repo {repo_id}", exception=e)
        raise e
    finally:
        PROCESSING_EVENTS.discard(record_id)


async def process_new_pipeline(pipeline_record: dict):
    """
    Process a new or updated pipeline run.
    """
    repo_id = pipeline_record['repo_id']
    pr_number = pipeline_record['pr_number']
    record_id = pipeline_record['id']
    
    if record_id in PROCESSING_EVENTS:
        logger.warning(f"Pipeline record {record_id} is already being processed. Skipping.")
        return
    
    PROCESSING_EVENTS.add(record_id)
    
    try:
        logger.info(f"Processing pipeline update for PR #{pr_number} in repository {repo_id}")
        
        # Broadcast pipeline state change
        await websocket_manager.broadcast_pr_state_update(repo_id, pr_number)
        
    except Exception as e:
        logger.error(f"Failed to process pipeline for PR #{pr_number} in repo {repo_id}", exception=e)
        raise e
    finally:
        PROCESSING_EVENTS.discard(record_id)


async def process_new_insight(insight_record: dict):
    """
    Process a new insight (usually generated by AI).
    """
    repo_id = insight_record['repo_id']
    pr_number = insight_record['pr_number']
    record_id = insight_record['id']
    
    if record_id in PROCESSING_EVENTS:
        logger.warning(f"Insight record {record_id} is already being processed. Skipping.")
        return
    
    PROCESSING_EVENTS.add(record_id)
    
    try:
        logger.info(f"Processing new insight for PR #{pr_number} in repository {repo_id}")
        
        # Broadcast insight update
        await websocket_manager.broadcast_pr_state_update(repo_id, pr_number)
        
    except Exception as e:
        logger.error(f"Failed to process insight for PR #{pr_number} in repo {repo_id}", exception=e)
        raise e
    finally:
        PROCESSING_EVENTS.discard(record_id)


async def _generate_ai_insight_for_pr(pr_record: dict):
    """
    Generate AI insights for a PR using the files_changed data.
    Returns True if insights were generated, False otherwise.
    """
    try:
        repo_id = pr_record['repo_id']
        pr_number = pr_record['pr_number']
        
        # Check if we have files_changed data
        files_changed = pr_record.get('files_changed', [])
        if not files_changed:
            logger.info(f"No files_changed data for PR #{pr_number}, skipping AI insight generation")
            return False
        
        # Generate AI insights using the AI service
        logger.info(f"Generating AI insights for PR #{pr_number} with {len(files_changed)} changed files")
        ai_insight_json = await ai_service.get_ai_insights(pr_record)
        
        if ai_insight_json:
            # Store the insights in the database with processed=false initially
            insight_record = {
                "repo_id": repo_id,
                "pr_number": pr_number,
                "commit_sha": pr_record.get("commit_sha"),
                "author": pr_record.get("author"),
                "avatar_url": pr_record.get("author_avatar"),
                "risk_level": ai_insight_json.get("risk_level", ai_insight_json.get("riskLevel", "low")).lower(),
                "summary": ai_insight_json.get("summary"),
                "recommendation": ai_insight_json.get("recommendation"),
                "processed": False  # Will be processed by the poller
            }
            
            await db_helpers.insert("insights", insight_record)
            logger.success(f"Generated and saved AI insight for PR #{pr_number} in repository {repo_id}")
            return True
        else:
            logger.warning(f"AI insight generation failed for PR #{pr_number}")
            return False
            
    except Exception as e:
        logger.error(f"Failed to generate AI insight for PR #{pr_number}", exception=e)
        return False